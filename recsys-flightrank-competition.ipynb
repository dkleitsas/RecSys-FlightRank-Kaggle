{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":105399,"databundleVersionId":12733338,"sourceType":"competition"}],"dockerImageVersionId":31042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AeroClub RecSys 2025 - XGBoost, LightGBM and combined Ensemble methods\n\nThis notebook implements a ranking approach using XGBoost, LightGBM or an ensemble and Polars for the AeroClub recommendation challenge.","metadata":{}},{"cell_type":"code","source":"!pip install shap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:51:55.952638Z","iopub.execute_input":"2025-08-16T08:51:55.952858Z","iopub.status.idle":"2025-08-16T08:52:08.451832Z","shell.execute_reply.started":"2025-08-16T08:51:55.952834Z","shell.execute_reply":"2025-08-16T08:52:08.447943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install optuna\n!pip install optuna-integration[lightgbm]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:52:08.454548Z","iopub.execute_input":"2025-08-16T08:52:08.454809Z","iopub.status.idle":"2025-08-16T08:52:20.957113Z","shell.execute_reply.started":"2025-08-16T08:52:08.454787Z","shell.execute_reply":"2025-08-16T08:52:20.952689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U xgboost\n!pip install -U polars\n!pip install -U lightgbm\n!pip install -U sklearn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:52:20.959814Z","iopub.execute_input":"2025-08-16T08:52:20.960044Z","iopub.status.idle":"2025-08-16T08:52:39.932690Z","shell.execute_reply.started":"2025-08-16T08:52:20.960021Z","shell.execute_reply":"2025-08-16T08:52:39.928720Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import polars as pl\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport xgboost as xgb\nimport lightgbm as lgb\n\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:52:39.934047Z","iopub.execute_input":"2025-08-16T08:52:39.934261Z","iopub.status.idle":"2025-08-16T08:52:44.338969Z","shell.execute_reply.started":"2025-08-16T08:52:39.934236Z","shell.execute_reply":"2025-08-16T08:52:44.334258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data\ntrain = pl.read_parquet('/kaggle/input/aeroclub-recsys-2025/train.parquet').drop('__index_level_0__')\ntest = pl.read_parquet('/kaggle/input/aeroclub-recsys-2025/test.parquet').drop('__index_level_0__').with_columns(pl.lit(0, dtype=pl.Int64).alias(\"selected\"))\n\ndata_raw = pl.concat((train, test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:52:44.341146Z","iopub.execute_input":"2025-08-16T08:52:44.341480Z","iopub.status.idle":"2025-08-16T08:52:49.064541Z","shell.execute_reply.started":"2025-08-16T08:52:44.341457Z","shell.execute_reply":"2025-08-16T08:52:49.060125Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helpers","metadata":{}},{"cell_type":"code","source":"def hitrate_at_3(y_true, y_pred, groups):\n    df = pl.DataFrame({\n        'group': groups,\n        'pred': y_pred,\n        'true': y_true\n    })\n    \n    return (\n        df.filter(pl.col(\"group\").count().over(\"group\") > 10)\n        .sort([\"group\", \"pred\"], descending=[False, True])\n        .group_by(\"group\", maintain_order=True)\n        .head(3)\n        .group_by(\"group\")\n        .agg(pl.col(\"true\").max())\n        .select(pl.col(\"true\").mean())\n        .item()\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:52:49.066686Z","iopub.execute_input":"2025-08-16T08:52:49.066946Z","iopub.status.idle":"2025-08-16T08:52:49.076192Z","shell.execute_reply.started":"2025-08-16T08:52:49.066922Z","shell.execute_reply":"2025-08-16T08:52:49.072214Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"df = data_raw.clone()\n\n# More efficient duration to minutes converter\ndef dur_to_min(col):\n    # Extract days and time parts in one pass\n    days = col.str.extract(r\"^(\\d+)\\.\", 1).cast(pl.Int64).fill_null(0) * 1440\n    time_str = pl.when(col.str.contains(r\"^\\d+\\.\")).then(col.str.replace(r\"^\\d+\\.\", \"\")).otherwise(col)\n    hours = time_str.str.extract(r\"^(\\d+):\", 1).cast(pl.Int64).fill_null(0) * 60\n    minutes = time_str.str.extract(r\":(\\d+):\", 1).cast(pl.Int64).fill_null(0)\n    return (days + hours + minutes).fill_null(0)\n    \n\ndur_cols = [\"legs0_duration\", \"legs1_duration\"] + [f\"legs{l}_segments{s}_duration\" for l in (0, 1) for s in range(0, 4)]\ndur_exprs = [dur_to_min(pl.col(c)).alias(c) for c in dur_cols if c in df.columns]\n\nif dur_exprs:\n    df = df.with_columns(dur_exprs)\n\n\nmc_cols = [f'legs{l}_segments{s}_marketingCarrier_code' for l in (0, 1) for s in range(4)]\nmc_exists = [col for col in mc_cols if col in df.columns]\n\ndf = df.with_columns([\n    \n        (pl.col(\"totalPrice\") / (pl.col(\"taxes\") + 1)).alias(\"price_per_tax\"),\n        (pl.col(\"taxes\") / (pl.col(\"totalPrice\") + 1)).alias(\"tax_rate\"),\n        pl.col(\"totalPrice\").log1p().alias(\"log_price\"),\n    \n        (pl.col(\"legs0_duration\").fill_null(0) + pl.col(\"legs1_duration\").fill_null(0)).alias(\"total_duration\"),\n    \n        pl.when(pl.col(\"legs1_duration\").fill_null(0) > 0)\n            .then(pl.col(\"legs0_duration\") / (pl.col(\"legs1_duration\") + 1))\n            .otherwise(1.0).alias(\"duration_ratio\"),\n    \n        (pl.col(\"legs1_duration\").is_null() | \n         (pl.col(\"legs1_duration\") == 0) | \n         pl.col(\"legs1_segments0_departureFrom_airport_iata\").is_null()).cast(pl.Int32).alias(\"is_one_way\"),\n    \n        (pl.sum_horizontal(pl.col(col).is_not_null().cast(pl.UInt8) for col in mc_exists) \n         if mc_exists else pl.lit(0)).alias(\"segments_count\"),\n    \n        (pl.col(\"frequentFlyer\").fill_null(\"\").str.count_matches(\"/\") + \n         (pl.col(\"frequentFlyer\").fill_null(\"\") != \"\").cast(pl.Int32)).alias(\"n_ff_programs\"),\n\n        pl.col(\"corporateTariffCode\").is_not_null().cast(pl.Int32).alias(\"has_corporate_tariff\"),\n        (pl.col(\"pricingInfo_isAccessTP\") == 1).cast(pl.Int32).alias(\"has_access_tp\"),\n\n        (pl.col(\"legs0_segments0_baggageAllowance_quantity\").fill_null(0) + \n         pl.col(\"legs1_segments0_baggageAllowance_quantity\").fill_null(0)).alias(\"baggage_total\"),\n        (pl.col(\"miniRules0_monetaryAmount\").fill_null(0) + \n         pl.col(\"miniRules1_monetaryAmount\").fill_null(0)).alias(\"total_fees\"),\n        \n        pl.col(\"searchRoute\").is_in([\"MOWLED/LEDMOW\", \"LEDMOW/MOWLED\", \"MOWLED\", \"LEDMOW\", \"MOWAER/AERMOW\"])\n            .cast(pl.Int32).alias(\"is_popular_route\"),\n    \n        pl.mean_horizontal([\"legs0_segments0_cabinClass\", \"legs1_segments0_cabinClass\"]).alias(\"avg_cabin_class\"),\n        (pl.col(\"legs0_segments0_cabinClass\").fill_null(0) - \n         pl.col(\"legs1_segments0_cabinClass\").fill_null(0)).alias(\"cabin_class_diff\"),\n])\n\n\ntotal_duration_expr = pl.sum_horizontal(pl.col(r\"^legs\\d+_duration$\")).fill_null(0)\n\ntotal_flight_time_expr = pl.sum_horizontal(pl.col(r\"^legs\\d+_segments\\d+_duration$\")).fill_null(0)\n\ntotal_wait_time_expr = total_duration_expr - total_flight_time_expr\n\nwait_time_ratio_expr = pl.when(total_duration_expr > 0)\\\n                         .then(total_wait_time_expr / total_duration_expr)\\\n                         .otherwise(0.0)\n\n\ndf = df.with_columns(\n    total_wait_time = total_wait_time_expr,\n    wait_time_ratio = wait_time_ratio_expr,\n)\n\n\ndf = df.with_columns(\n    days_to_departure=(pl.col(\"legs0_departureAt\").str.to_datetime() - pl.col(\"requestDate\")).dt.total_days()\n).with_columns(\n    last_minute_flight=((pl.col(\"days_to_departure\") <= 30).cast(pl.Int8)),\n    flight_advance_bucket=pl.when(pl.col(\"days_to_departure\").is_between(0, 7))\n      .then(pl.lit(0, dtype=pl.UInt8))\n    .when(pl.col(\"days_to_departure\").is_between(8, 30))\n      .then(pl.lit(1, dtype=pl.UInt8))\n    .when(pl.col(\"days_to_departure\").is_between(31, 90))\n      .then(pl.lit(2, dtype=pl.UInt8))\n    .otherwise(pl.lit(3, dtype=pl.UInt8))\n)\n\ncarrier_cols = df.select(pl.col(\"^legs.*_marketingCarrier_code$\")).columns\n\ndf_with_ratio = df.with_columns(\n    matching_segments_count = pl.sum_horizontal(\n        [\n            pl.col(c).is_in(pl.col(\"frequentFlyer\").str.split(\"/\"))\n            for c in carrier_cols\n        ]\n    ).fill_null(0)\n\n).with_columns(\n    ff_match_ratio = (\n        pl.col(\"matching_segments_count\") / pl.col(\"segments_count\")\n    ).fill_nan(0)\n)\n\n\ndef process_fare_rule(df, rule_index, price_col):\n    status_col = f\"miniRules{rule_index}_statusInfos\"\n    money_col = f\"miniRules{rule_index}_monetaryAmount\"\n    percent_col = f\"miniRules{rule_index}_percentage\"\n\n\n    df_processed = df.with_columns(\n        pl.when(pl.col(status_col) == 0)\n          .then(pl.col(price_col))\n          .otherwise(pl.col(money_col))\n          .alias(money_col),\n\n        pl.when(pl.col(status_col) == 0)\n          .then(pl.lit(100.0))\n          .otherwise(pl.col(percent_col))\n          .alias(percent_col)\n\n    ).with_columns(\n        pl.when(pl.col(percent_col).is_null())\n          .then((pl.col(money_col) / pl.col(price_col)) * 100)\n          .otherwise(pl.col(percent_col))\n          .alias(percent_col)\n\n    ).with_columns(\n        pl.when(pl.col(money_col).is_null())\n          .then((pl.col(percent_col) / 100) * pl.col(price_col))\n          .otherwise(pl.col(money_col))\n          .alias(money_col)\n    )\n\n    return df_processed\n\n\ndf = process_fare_rule(df, rule_index=0, price_col=\"totalPrice\")\ndf = process_fare_rule(df, rule_index=1, price_col=\"totalPrice\")\n\ndf = (\n    df.join(\n        train.group_by('legs0_segments0_marketingCarrier_code').agg(pl.mean('selected').alias('carrier0_pop')),\n        on='legs0_segments0_marketingCarrier_code', \n        how='left'\n    )\n    .join(\n        train.group_by('legs1_segments0_marketingCarrier_code').agg(pl.mean('selected').alias('carrier1_pop')),\n        on='legs1_segments0_marketingCarrier_code', \n        how='left'\n    )\n    .with_columns([\n        pl.col('carrier0_pop').fill_null(0.0),\n        pl.col('carrier1_pop').fill_null(0.0),\n    ])\n)\n\ndf = df.with_columns([\n    pl.when(pl.col('legs1_segments0_marketingCarrier_code').is_null())\n      .then(pl.col('carrier0_pop'))\n      .otherwise(pl.col('carrier0_pop') * pl.col('carrier1_pop'))\n      .alias('carrier_pop_combined')\n])\n\n\nfor leg_idx in range(2):\n    for seg_idx in range(4):\n        \n        qty_col = f\"legs_{leg_idx}_segments_{seg_idx}_baggageAllowance_quantity\"\n        type_col = f\"legs_{leg_idx}_segments_{seg_idx}_baggageAllowance_weightMeasurementType\"\n        \n        if qty_col in df.columns:\n            \n            df = df.with_columns(\n                pl.when(pl.col(type_col) == 0)\n                  .then(pl.col(qty_col).fill_null(0) * 20)\n                  .otherwise(pl.col(qty_col).fill_null(0))\n                  .alias(qty_col) # Overwrite the original quantity column\n            )\n\n\ncols_to_drop = df.select(pl.col(\"^.*weightMeasurementType$\")).columns\ndf = df.drop(cols_to_drop)\n\n\nseg_exprs = []\nfor leg in (0, 1):\n    seg_cols = [f\"legs{leg}_segments{s}_duration\" for s in range(4) if f\"legs{leg}_segments{s}_duration\" in df.columns]\n    if seg_cols:\n        seg_exprs.append(\n            pl.sum_horizontal(pl.col(c) != 0 for c in seg_cols)\n                .cast(pl.Int32).alias(f\"n_segments_leg{leg}\")\n        )\n    else:\n        seg_exprs.append(pl.lit(0).cast(pl.Int32).alias(f\"n_segments_leg{leg}\"))\n\ndf = df.with_columns(seg_exprs)\n\n\ndf = df.with_columns([\n    (pl.col(\"n_segments_leg0\") == 1).cast(pl.Int32).alias(\"is_direct_leg0\"),\n    pl.when(pl.col(\"is_one_way\") == 1).then(0)\n        .otherwise((pl.col(\"n_segments_leg1\") == 1).cast(pl.Int32)).alias(\"is_direct_leg1\"),\n    pl.col(\"Id\").count().over(\"ranker_id\").alias(\"group_size\"),\n])\n\ndf = df.with_columns([\n    (pl.col(\"is_direct_leg0\") & pl.col(\"is_direct_leg1\")).cast(pl.Int32).alias(\"both_direct\"),\n    ((pl.col(\"isVip\") == 1) | (pl.col(\"n_ff_programs\") > 0)).cast(pl.Int32).alias(\"is_vip_freq\"),\n    (pl.col(\"baggage_total\") > 0).cast(pl.Int32).alias(\"has_baggage\"),\n    (pl.col(\"total_fees\") > 0).cast(pl.Int32).alias(\"has_fees\"),\n    (pl.col(\"total_fees\") / (pl.col(\"totalPrice\") + 1)).alias(\"fee_rate\"),\n    pl.col(\"group_size\").log1p().alias(\"group_size_log\"),\n    (pl.col(\"total_duration\") == (pl.col(\"total_duration\")).min().over(\"ranker_id\")).cast(pl.Int32).alias(\"is_fastest\"),\n    pl.col(\"legs0_duration\") == pl.col(\"legs0_duration\").min().over(\"ranker_id\").cast(pl.Int32).alias(\"is_leg0_fastest\"),\n    (pl.col(\"segments_count\") == pl.col(\"segments_count\").min().over(\"ranker_id\")).cast(pl.Int32).alias(\"is_min_segments\"),\n])\n\n\nif \"legs0_segments0_marketingCarrier_code\" in df.columns:\n    df = df.with_columns(\n        pl.col(\"legs0_segments0_marketingCarrier_code\").is_in([\"SU\", \"S7\", \"U6\"])\n            .cast(pl.Int32).alias(\"is_major_carrier\"))\nelse:\n    df = df.with_columns(pl.lit(0).alias(\"is_major_carrier\"))\n    \n\ntime_exprs = []\nfor col in (\"legs0_departureAt\", \"legs0_arrivalAt\", \"legs1_departureAt\", \"legs1_arrivalAt\"):\n    if col in df.columns:\n        dt = pl.col(col).str.to_datetime(strict=False)\n        h = dt.dt.hour().fill_null(12)\n        time_segment = (h // 4)\n        time_exprs.extend([\n            h.alias(f\"{col}_hour\"),\n            dt.dt.weekday().fill_null(0).alias(f\"{col}_weekday\"),\n            (((h >= 6) & (h <= 9)) | ((h >= 17) & (h <= 20))).cast(pl.Int32).alias(f\"{col}_business_time\"),\n            time_segment.alias(f\"{col}_time_segment\")\n        ])\nif time_exprs:\n    df = df.with_columns(time_exprs)\n\n\nrank_exprs = []\nfor col, alias in [\n    ('totalPrice',           'price'),\n    ('total_duration',       'duration'),\n    ('avg_cabin_class',      'cabin_class'),\n    ('total_fees',           'fees'),\n    ('segments_count',       'segments'),\n    ('taxes',                'taxes'),\n    ('carrier_pop_combined', 'carrier_pop')\n]:\n    raw   = pl.col(col).rank().over('ranker_id')\n    norm  = (raw - 1) / (pl.col('group_size') - 1)\n    rank_exprs += [\n        raw.alias(f'{alias}_rank'),\n        norm.alias(f'{alias}_rank_norm')\n    ]\n\ndf = df.with_columns(rank_exprs)\n\ndf = df.with_columns(\n    ((\n        pl.col('price_rank_norm') +\n        pl.col('duration_rank_norm') +\n        pl.col('cabin_class_rank_norm')\n    ) / 3)\n    .alias('combined_rank_norm')\n)\n\nfeatures = []\nfor col in ['totalPrice','total_duration', 'avg_cabin_class', 'total_fees', 'taxes']:\n    raw   = pl.col(col)\n    rmin  = (raw / (raw.min().over('ranker_id') + 1e-6)).alias(f'{col}_ratio_min')\n    delt  = (raw - raw.min().over('ranker_id')).alias(f'{col}_delta_min')\n    qtl   = (raw.rank().over('ranker_id') - 1) / (pl.col('group_size') - 1)\n    features += [rmin, delt, qtl.alias(f'{col}_qtl')]\n\ndf = df.with_columns(features)\n\nprice_exprs = [\n    (pl.col(\"totalPrice\") == pl.col(\"totalPrice\").min().over(\"ranker_id\")).cast(pl.Int32).alias(\"is_cheapest\"),\n    ((pl.col(\"totalPrice\") - pl.col(\"totalPrice\").median().over(\"ranker_id\")) / \n    (pl.col(\"totalPrice\") / pl.col(\"total_duration\").alias(\"price_per_duration\")))\n]\n\ndf = df.with_columns(price_exprs)\n\ndf = df.with_columns(\n    (\n        # Policy compliance (25% weight)\n        (pl.col(\"pricingInfo_isAccessTP\") * 0.25) +\n        # Direct flights (25% weight)\n        (pl.col(\"is_direct_leg0\") * 0.25) +\n        # Business-hour departures/arrivals (25% weight)\n        ((pl.col(\"legs0_departureAt_business_time\") + pl.col(\"legs1_departureAt_business_time\")) * 0.125) +\n        # VIP preference for business class (25% weight)\n        ((pl.col(\"isVip\") == 1) & (pl.col(\"avg_cabin_class\") >= 1.5)).cast(pl.Int8) * 0.25\n    ).alias(\"business_traveler_perfect_match\"),\n\n    # Timezone diff only\n    (pl.col(\"legs0_arrivalAt_hour\") - pl.col(\"legs0_departureAt_hour\") -\n     (pl.col(\"legs0_duration\") / 60)).alias(\"timezone_diff_leg0\"),\n)\n\ndf = df.with_columns(\n    (\n        (pl.col(\"is_one_way\") == 0) &  # Round-trip\n        (pl.col(\"legs0_arrivalAt_hour\") >= 8) &  # Arrive by morning\n        (pl.col(\"legs1_departureAt_hour\") <= 18) &  # Return by evening\n        (pl.col(\"timezone_diff_leg0\").abs() < 3)   # Minimal jetlag\n    ).cast(pl.Int8).alias(\"meeting_friendly_itinerary\")\n)\n\nis_direct_cheapest_expr = (\n    (pl.col(\"is_direct_leg0\") == 1) &\n    (pl.col(\"totalPrice\") == \n        pl.when(pl.col(\"is_direct_leg0\") == 1)\n          .then(pl.col(\"totalPrice\"))\n          .min()\n          .over(\"ranker_id\")\n    )\n).cast(pl.Int32).fill_null(0).alias(\"is_direct_cheapest\")\n\nwithin_TP_highest_class_expr = (\n    (pl.col(\"pricingInfo_isAccessTP\") == 1) &\n    (pl.col(\"avg_cabin_class\") == \n        pl.when(pl.col(\"pricingInfo_isAccessTP\") == 1)\n          .then(pl.col(\"avg_cabin_class\"))\n           .max()\n          .over(\"ranker_id\")\n    )\n).cast(pl.Int32).fill_null(0).alias(\"within_TP_highest_class\")\n\nis_cheapest_corporate_expr = (\n    (pl.col(\"corporateTariffCode\").is_not_null()) &\n    (pl.col(\"totalPrice\") == \n        pl.when(pl.col(\"corporateTariffCode\").is_not_null())\n          .then(pl.col(\"totalPrice\"))\n          .min()\n          .over(\"ranker_id\")\n    )\n).cast(pl.Int32).fill_null(0).alias(\"is_cheapest_corporate\")\n\nis_highest_class_corporate_expr = (\n    (pl.col(\"corporateTariffCode\").is_not_null()) &\n    (pl.col(\"avg_cabin_class\") == \n        pl.when(pl.col(\"corporateTariffCode\").is_not_null())\n          .then(pl.col(\"avg_cabin_class\"))\n          .max()\n          .over(\"ranker_id\")\n    )\n).cast(pl.Int32).fill_null(0).alias(\"is_highest_class_corporate\")\n\nis_free_cancel = (\n    (pl.col(\"miniRules0_statusInfos\") == 1) &\n    (pl.col(\"miniRules0_monetaryAmount\") == 0)\n).fill_null(False).alias(\"free_cancel\")\n\nis_free_exchange = (\n    (pl.col(\"miniRules1_statusInfos\") == 1) &\n    (pl.col(\"miniRules1_monetaryAmount\") == 0)\n    ).fill_null(False).alias(\"free_exchange\")\n\n\n# --- Execute them all at once ---\ndf = df.with_columns(\n    is_direct_cheapest_expr,\n    within_TP_highest_class_expr,\n    is_cheapest_corporate_expr,\n    is_highest_class_corporate_expr,\n    is_free_cancel,\n    is_free_exchange,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:52:49.078815Z","iopub.execute_input":"2025-08-16T08:52:49.079052Z","iopub.status.idle":"2025-08-16T08:54:17.543718Z","shell.execute_reply.started":"2025-08-16T08:52:49.079031Z","shell.execute_reply":"2025-08-16T08:54:17.537368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill nulls\ndata = df.with_columns(\n    [pl.col(c).fill_null(0) for c in df.select(pl.selectors.numeric()).columns] +\n    [pl.col(c).fill_null(\"missing\") for c in df.select(pl.selectors.string()).columns]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:54:17.545829Z","iopub.execute_input":"2025-08-16T08:54:17.546351Z","iopub.status.idle":"2025-08-16T08:54:19.898266Z","shell.execute_reply.started":"2025-08-16T08:54:17.546325Z","shell.execute_reply":"2025-08-16T08:54:19.891936Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Selection","metadata":{}},{"cell_type":"code","source":"# Categorical features\ncat_features = [\n    'nationality', 'searchRoute', 'corporateTariffCode',\n    'bySelf', 'sex', 'companyID',\n    # Leg 0 segments 0-1\n    'legs0_segments0_aircraft_code', 'legs0_segments0_arrivalTo_airport_city_iata',\n    'legs0_segments0_arrivalTo_airport_iata', 'legs0_segments0_departureFrom_airport_iata',\n    'legs0_segments0_marketingCarrier_code', 'legs0_segments0_operatingCarrier_code',\n    'legs0_segments0_flightNumber',\n    'legs0_segments1_aircraft_code', 'legs0_segments1_arrivalTo_airport_city_iata',\n    'legs0_segments1_arrivalTo_airport_iata', 'legs0_segments1_departureFrom_airport_iata',\n    'legs0_segments1_marketingCarrier_code', 'legs0_segments1_operatingCarrier_code',\n    'legs0_segments1_flightNumber',\n    # Leg 1 segments 0-1\n    'legs1_segments0_aircraft_code', 'legs1_segments0_arrivalTo_airport_city_iata',\n    'legs1_segments0_arrivalTo_airport_iata', 'legs1_segments0_departureFrom_airport_iata',\n    'legs1_segments0_marketingCarrier_code', 'legs1_segments0_operatingCarrier_code',\n    'legs1_segments0_flightNumber',\n    'legs1_segments1_aircraft_code', 'legs1_segments1_arrivalTo_airport_city_iata',\n    'legs1_segments1_arrivalTo_airport_iata', 'legs1_segments1_departureFrom_airport_iata',\n    'legs1_segments1_marketingCarrier_code', 'legs1_segments1_operatingCarrier_code',\n    'legs1_segments1_flightNumber',\n]\n\nte_features = [\n    'corporateTariffCode',\n    # Leg 0 segments 0-1\n    'legs0_segments0_aircraft_code',\n    'legs0_segments0_marketingCarrier_code', 'legs0_segments0_operatingCarrier_code',\n    'legs0_segments1_aircraft_code',\n    'legs0_segments1_marketingCarrier_code', 'legs0_segments1_operatingCarrier_code',\n    # Leg 1 segments 0-1\n    'legs1_segments0_aircraft_code',\n    'legs1_segments0_marketingCarrier_code', 'legs1_segments0_operatingCarrier_code',\n    'legs1_segments1_aircraft_code',\n    'legs1_segments1_marketingCarrier_code', 'legs1_segments1_operatingCarrier_code',\n]\n\n# Columns to exclude (uninformative or problematic)\nexclude_cols = [\n    'Id', 'ranker_id', 'selected', 'profileId', 'requestDate', \"pricingInfo_isAccessTP\",\n    'legs0_departureAt', 'legs0_arrivalAt', 'legs1_departureAt', 'legs1_arrivalAt', \"bySelf\",\n    'frequentFlyer', 'pricingInfo_passengerCount'\n]\n\n\n# Exclude segment 2-3 columns (>98% missing)\nfor leg in [0, 1]:\n    for seg in [2, 3]:\n        for suffix in ['aircraft_code', 'arrivalTo_airport_city_iata', 'arrivalTo_airport_iata',\n                      'baggageAllowance_quantity',\n                      'cabinClass', 'departureFrom_airport_iata', 'duration', 'flightNumber',\n                      'marketingCarrier_code', 'operatingCarrier_code', 'seatsAvailable']:\n            exclude_cols.append(f'legs{leg}_segments{seg}_{suffix}')\n\n\nfor leg in [0, 1]:\n    for seg in [0, 1]:\n        if seg == 0:\n            suffixes = [\n                \"seatsAvailable\",\n            ]\n        else:\n            suffixes = [\n                \"cabinClass\",\n                \"seatsAvailable\",\n                \"baggageAllowance_quantity\",\n                \"baggageAllowance_weightMeasurementType\",\n                \"aircraft_code\",\n                \"arrivalTo_airport_city_iata\",\n                \"arrivalTo_airport_iata\",\n                \"departureFrom_airport_iata\",\n                \"flightNumber\",\n                \"marketingCarrier_code\",\n                \"operatingCarrier_code\",\n            ]\n        for suffix in suffixes:\n            exclude_cols.append(f\"legs{leg}_segments{seg}_{suffix}\")\n\n\nfeature_cols = [col for col in data.columns if col not in exclude_cols]\ncat_features_final = [col for col in cat_features if col in feature_cols]\n\nprint(feature_cols)\nprint(cat_features_final)\nprint(f\"Using {len(feature_cols)} features ({len(cat_features_final)} categorical)\")\n\n# Not used\nsmall_model_features = [\n    \"totalPrice\",\n    \"total_duration\",\n    \"fee_rate\",\n\n    \"is_one_way\",\n    \"segments_count\",\n    \"n_segments_leg0\",\n    \"n_segments_leg1\",\n    \"is_direct_leg0\",\n    \"is_direct_leg1\",\n    \"is_min_segments\",\n\n    \"is_major_carrier\",\n    \"avg_cabin_class\",\n    \"has_baggage\",\n    \"has_access_tp\",\n    \"free_exchange\",\n    \"free_cancel\",\n    \"corporateTariffCode\",\n    \"within_TP_highest_class\",\n\n    \"days_to_departure\",\n    \"legs0_departureAt_hour\",\n    \"legs0_departureAt_weekday\",\n    \"legs0_departureAt_business_time\",\n    \"legs0_arrivalAt_hour\",\n    \"legs1_departureAt_hour\",\n\n    \"price_rank_norm\",\n    \"duration_rank_norm\",\n    \"cabin_class_rank_norm\",\n\n    \"total_duration_delta_min\",\n    \"totalPrice_delta_min\",\n\n]\n\nX = data.select(feature_cols)\ny = data.select('selected')\ngroups = data.select('ranker_id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:54:19.900368Z","iopub.execute_input":"2025-08-16T08:54:19.900651Z","iopub.status.idle":"2025-08-16T08:54:20.036625Z","shell.execute_reply.started":"2025-08-16T08:54:19.900626Z","shell.execute_reply":"2025-08-16T08:54:19.918180Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"data_final = X.with_columns([(pl.col(c).rank(\"dense\") - 1).fill_null(-1).cast(pl.Int32) for c in cat_features_final])\n\nn1 = 16487352 # split train to train and val (10%) in time\nn2 = train.height\ndata_tr, data_va, data_te = data_final[:n1], data_final[n1:n2], data_final[n2:]\ny_tr, y_va, y_te = y[:n1], y[n1:n2], y[n2:]\ngroups_tr, groups_va, groups_te = groups[:n1], groups[n1:n2], groups[n2:]\n\ntarget_col = \"selected\" \nprior = y_tr.select(pl.col(y_tr.columns[0]).mean()).item()\n\ndf_tr = data_tr.with_columns(pl.Series(name=target_col, values=y_tr))\n\nfor c in te_features:\n    if c not in df_tr.columns:\n        continue\n\n    mp = (\n        df_tr.group_by(c)\n             .agg([\n                 pl.len().alias(\"cnt\"),\n                 pl.col(target_col).sum().alias(\"sum_y\")\n             ])\n             .with_columns(\n                 ((pl.col(\"sum_y\") + 10 * prior) / (pl.col(\"cnt\") + 10)).alias(f\"{c}_te\")\n             )\n             .select([c, f\"{c}_te\"])\n    )\n\n    data_tr = (data_tr.join(mp, on=c, how=\"left\").with_columns(pl.col(f\"{c}_te\").fill_null(prior)))\n    data_va = (data_va.join(mp, on=c, how=\"left\").with_columns(pl.col(f\"{c}_te\").fill_null(prior)))\n    data_te = (data_te.join(mp, on=c, how=\"left\").with_columns(pl.col(f\"{c}_te\").fill_null(prior)))\n\n\ny_tr = y_tr[\"selected\"].to_numpy()\ny_va = y_va[\"selected\"].to_numpy()\ny_te = y_te[\"selected\"].to_numpy()\n\ngroup_sizes_tr = groups_tr.group_by('ranker_id', maintain_order=True).agg(pl.len())['len'].to_numpy()\ngroup_sizes_va = groups_va.group_by('ranker_id', maintain_order=True).agg(pl.len())['len'].to_numpy()\ngroup_sizes_te = groups_te.group_by('ranker_id', maintain_order=True).agg(pl.len())['len'].to_numpy()\n\ndtrain_xgb = xgb.DMatrix(data_tr, label=y_tr, group=group_sizes_tr)\ndval_xgb   = xgb.DMatrix(data_va, label=y_va, group=group_sizes_va)\ndtest_xgb  = xgb.DMatrix(data_te, label=y_te, group=group_sizes_te)\n\ndtrain_lgb = lgb.Dataset(data_tr, label=y_tr, group=group_sizes_tr)\ndval_lgb   = lgb.Dataset(data_va, label=y_va, group=group_sizes_va, reference=dtrain_lgb)\ndtest_lgb  = lgb.Dataset(data_te, label=y_te, group=group_sizes_te, reference=dtrain_lgb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:54:20.037645Z","iopub.execute_input":"2025-08-16T08:54:20.037907Z","iopub.status.idle":"2025-08-16T08:55:10.809256Z","shell.execute_reply.started":"2025-08-16T08:54:20.037866Z","shell.execute_reply":"2025-08-16T08:55:10.804724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_type = \"ensemble\"\ntuning = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:55:10.811502Z","iopub.execute_input":"2025-08-16T08:55:10.811762Z","iopub.status.idle":"2025-08-16T08:55:10.821745Z","shell.execute_reply.started":"2025-08-16T08:55:10.811738Z","shell.execute_reply":"2025-08-16T08:55:10.816529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if tuning:\n\n    import optuna\n    import functools\n    \n    if model_type == \"lgb\":\n        \n        from optuna.integration import LightGBMPruningCallback\n        \n        def objective(trial, dtrain, dval):\n            params = {\n                'objective': 'lambdarank',\n                'metric': 'ndcg',\n                'eval_at': [3],\n                'boosting_type': 'gbdt',\n                'seed': RANDOM_STATE,\n                'n_jobs': -1,\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n                'num_leaves': trial.suggest_int('num_leaves', 31, 150),\n                'max_depth': trial.suggest_int('max_depth', 5, 10),\n                'min_child_weight': trial.suggest_float('min_child_weight', 1, 10),\n                'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),\n                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),\n                'bagging_freq': trial.suggest_int('bagging_freq', 1, 5),\n                'lambda_l2': trial.suggest_float('lambda_l2', 1e-2, 10.0, log=True),\n                'verbosity': -1\n            }\n        \n            model = lgb.train(\n                params,\n                dtrain,\n                num_boost_round=800,  # ↓ smaller during tuning\n                valid_sets=[dtrain, dval],\n                valid_names=['train', 'val'],\n                callbacks=[\n                    lgb.early_stopping(stopping_rounds=50, verbose=False),\n                    LightGBMPruningCallback(trial, \"ndcg@3\", valid_name=\"val\")\n                ]\n            )\n    \n            return model.best_score['val']['ndcg@3']\n        \n        # --- Run the study ---\n        study = optuna.create_study(direction='maximize')\n        objective_with_data = functools.partial(objective, dtrain=dtrain_lgb, dval=dval_lgb)\n        study.optimize(objective_with_data, n_trials=40, n_jobs=1)\n    \n        \n        # --- Print the results ---\n        print(\"\\n--- Optuna Study Results ---\")\n        print(f\"Number of finished trials: {len(study.trials)}\")\n        print(\"Best trial:\")\n        best_trial = study.best_trial\n        print(f\"  Value (NDCG@3): {best_trial.value:.5f}\")\n        print(\"  Best Parameters: \")\n        for key, value in best_trial.params.items():\n            print(f\"    {key}: {value}\")\n\n    elif model_type == \"xgb\":\n\n        from optuna.integration import XGBoostPruningCallback\n        \n        def objective(trial, dtrain, dval):\n            params = {\n                'objective': 'rank:pairwise',   # XGBoost ranking objective\n                'eval_metric': 'ndcg@3',    # Same evaluation as LightGBM\n                'tree_method': 'hist',      # Fast, memory-efficient histogram algorithm\n                'seed': RANDOM_STATE,\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n                'max_depth': trial.suggest_int('max_depth', 5, 10),\n                'min_child_weight': trial.suggest_float('min_child_weight', 1, 10),\n                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n                'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n                'lambda': trial.suggest_float('lambda', 1e-2, 10.0, log=True),  # L2 regularization\n                'alpha': trial.suggest_float('alpha', 1e-2, 10.0, log=True),   # L1 regularization\n            }\n    \n            # Train the model\n            bst = xgb.train(\n                params,\n                dtrain,\n                num_boost_round=800,  # Can be smaller during tuning\n                evals=[(dtrain, 'train'), (dval, 'val')],\n                early_stopping_rounds=50,\n                verbose_eval=False,\n                callbacks=[\n                    XGBoostPruningCallback(trial, \"val-ndcg@3\")\n                ]\n            )\n        \n            # Return the best NDCG score\n            return bst.best_score\n        \n        # --- Run the study ---\n        study = optuna.create_study(direction='maximize')\n        objective_with_data = functools.partial(objective, dtrain=dtrain_xgb, dval=dval_xgb)\n        study.optimize(objective_with_data, n_trials=40, n_jobs=1)\n        \n        # --- Print the results ---\n        print(\"\\n--- Optuna Study Results ---\")\n        print(f\"Number of finished trials: {len(study.trials)}\")\n        print(\"Best trial:\")\n        best_trial = study.best_trial\n        print(f\"  Value (NDCG@3): {best_trial.value:.5f}\")\n        print(\"  Best Parameters: \")\n        for key, value in best_trial.params.items():\n            print(f\"    {key}: {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:55:10.823672Z","iopub.execute_input":"2025-08-16T08:55:10.823917Z","iopub.status.idle":"2025-08-16T08:55:10.842649Z","shell.execute_reply.started":"2025-08-16T08:55:10.823886Z","shell.execute_reply":"2025-08-16T08:55:10.837425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(model_type)\n\nif model_type == \"xgb\" or model_type == \"ensemble\":\n    xgb_params = {\n        'objective': 'rank:pairwise',\n        'eval_metric': 'ndcg@3',\n        'learning_rate': 0.06030516375230898,\n        'max_depth': 9,\n        'min_child_weight': 3.4021529387521423,\n        'colsample_bytree': 0.7437840878139231,\n        'subsample': 0.9493117941317905,\n        'lambda': 2.801483598481627,\n        'alpha': 0.531733951944211,\n        'seed': RANDOM_STATE,\n        'n_jobs': -1,\n        # 'device': 'cuda'\n    }\n    \n    # Train XGBoost model\n    print(\"Training XGBoost model...\")\n    xgb_model = xgb.train(\n        xgb_params,\n        dtrain_xgb,\n        num_boost_round=1500,\n        evals=[(dtrain_xgb, 'train'), (dval_xgb, 'val')],\n        early_stopping_rounds=50,\n        verbose_eval=50\n    )\nif model_type == \"lgb\" or model_type == \"ensemble\":\n    lgb_params = {\n        'objective': 'lambdarank',  \n        'metric': 'ndcg',\n        'eval_at': [3], \n        'boosting_type': 'gbdt',\n        'num_leaves': 255,  \n        'max_depth': 10,\n        'min_child_weight': 10,\n        'bagging_fraction': 0.8, \n        'feature_fraction': 0.8,\n        'lambda_l2': 10.0,\n        'learning_rate': 0.05,\n        'seed': RANDOM_STATE,\n        'n_jobs': -1,\n    }\n    \n    lgb_model = lgb.train(\n        lgb_params,\n        dtrain_lgb,\n        num_boost_round=1000,\n        valid_sets=[dtrain_lgb, dval_lgb],\n        valid_names=['train', 'val'],\n        callbacks=[lgb.log_evaluation(period=50), lgb.early_stopping(stopping_rounds=100)]\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T09:47:30.667359Z","iopub.execute_input":"2025-08-16T09:47:30.667737Z","iopub.status.idle":"2025-08-16T09:54:43.167273Z","shell.execute_reply.started":"2025-08-16T09:47:30.667707Z","shell.execute_reply":"2025-08-16T09:54:43.163089Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Performance analysis and visualizations","metadata":{}},{"cell_type":"code","source":"if model_type == \"xgb\":\n    # Evaluate XGBoost\n    xgb_va_preds = xgb_model.predict(dval_xgb)\n    xgb_hr3 = hitrate_at_3(y_va, xgb_va_preds, groups_va)\n    print(f\"HitRate@3: {xgb_hr3:.3f}\")\nelse:\n    # Evaluate LGBM\n    lgb_va_preds = lgb_model.predict(data_va)\n    lgb_hr3 = hitrate_at_3(y_va, lgb_va_preds, groups_va)\n    print(f\"HitRate@3: {lgb_hr3:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:55:10.862726Z","iopub.status.idle":"2025-08-16T08:55:10.864044Z","shell.execute_reply.started":"2025-08-16T08:55:10.862914Z","shell.execute_reply":"2025-08-16T08:55:10.862928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if model_type == \"xgb\":\n    xgb_importance = xgb_model.get_score(importance_type='gain')\n    xgb_importance_df = pl.DataFrame(\n        [{'feature': k, 'importance': v} for k, v in xgb_importance.items()]\n    ).sort('importance', descending=bool(1))\n    print(xgb_importance_df.head(100).to_pandas().to_string())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:55:10.865004Z","iopub.status.idle":"2025-08-16T08:55:10.865599Z","shell.execute_reply.started":"2025-08-16T08:55:10.865140Z","shell.execute_reply":"2025-08-16T08:55:10.865152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Trying out shap analysis for lgb\n\nif model_type == \"lgb\":\n\n    import shap \n    \n    X_sample = data_va.sample(100000)\n    X_np = X_sample.to_numpy()\n    \n    explainer = shap.TreeExplainer(lgb_model)\n    shap_values = explainer.shap_values(X_np)\n    \n    shap.summary_plot(shap_values, X_sample.to_pandas(), plot_type=\"bar\", max_display=200)\n    shap.summary_plot(shap_values, X_sample.to_pandas())\n    \n    \n    import numpy as np\n    \n    threshold = 0.1\n\n    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n    \n    feature_names = X_sample.columns\n    \n    low_importance_features = []\n    for i, score in enumerate(mean_abs_shap):\n        if score < threshold:\n            low_importance_features.append((feature_names[i], score))\n    \n    low_importance_features.sort(key=lambda x: x[1])\n    \n    if not low_importance_features:\n        print(f\"\\n✅ All features have a mean absolute SHAP value of {threshold} or greater.\")\n    else:\n        print(f\"\\nFound {len(low_importance_features)} features with a mean absolute SHAP value below {threshold}:\")\n        print(\"-\" * 70)\n        print(f\"{'Feature Name':<45} | {'Mean ABS SHAP Value'}\")\n        print(\"-\" * 70)\n        for name, score in low_importance_features:\n            print(f\"{name:<45} | {score:.6f}\")\n        print(\"-\" * 70)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:55:10.867378Z","iopub.status.idle":"2025-08-16T08:55:10.868380Z","shell.execute_reply.started":"2025-08-16T08:55:10.867537Z","shell.execute_reply":"2025-08-16T08:55:10.867552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analysis of xgb hit rate for different group sizes\n\nif model_type == \"xgb\":\n    # Color palette\n    red = (0.86, 0.08, 0.24)\n    blue = (0.12, 0.56, 1.0)\n    \n    # Prepare data for analysis\n    va_df = pl.DataFrame({\n        'ranker_id': groups_va.to_numpy().flatten(),\n        'pred_score': xgb_va_preds,\n        'selected': y_va.flatten()\n    })\n    \n    # Add group size and filter\n    va_df = va_df.join(\n        va_df.group_by('ranker_id').agg(pl.len().alias('group_size')), \n        on='ranker_id'\n    ).filter(pl.col('group_size') > 10)\n    \n    # Calculate group size quantiles\n    size_quantiles = va_df.select('ranker_id', 'group_size').unique().select(\n        pl.col('group_size').quantile(0.25).alias('q25'),\n        pl.col('group_size').quantile(0.50).alias('q50'),\n        pl.col('group_size').quantile(0.75).alias('q75')\n    ).to_dicts()[0]\n    \n    # Function to calculate hitrate curve efficiently\n    def calculate_hitrate_curve(df, k_values):\n        # Sort once and calculate all k values\n        sorted_df = df.sort([\"ranker_id\", \"pred_score\"], descending=[False, True])\n        return [\n            sorted_df.group_by(\"ranker_id\", maintain_order=True)\n            .head(k)\n            .group_by(\"ranker_id\")\n            .agg(pl.col(\"selected\").max().alias(\"hit\"))\n            .select(pl.col(\"hit\").mean())\n            .item()\n            for k in k_values\n        ]\n    \n    # Calculate curves\n    k_values = list(range(1, 21))\n    curves = {\n        'All groups (>10)': calculate_hitrate_curve(va_df, k_values),\n        f'Small (11-{int(size_quantiles[\"q25\"])})': calculate_hitrate_curve(\n            va_df.filter(pl.col('group_size') <= size_quantiles['q25']), k_values\n        ),\n        f'Medium ({int(size_quantiles[\"q25\"]+1)}-{int(size_quantiles[\"q75\"])})': calculate_hitrate_curve(\n            va_df.filter((pl.col('group_size') > size_quantiles['q25']) & \n                        (pl.col('group_size') <= size_quantiles['q75'])), k_values\n        ),\n        f'Large (>{int(size_quantiles[\"q75\"])})': calculate_hitrate_curve(\n            va_df.filter(pl.col('group_size') > size_quantiles['q75']), k_values\n        )\n    }\n    \n    # Calculate hitrate@3 by group size using log-scale bins\n    # Create log-scale bins\n    min_size = va_df['group_size'].min()\n    max_size = va_df['group_size'].max()\n    bins = np.logspace(np.log10(min_size), np.log10(max_size), 51)  # 51 edges = 50 bins\n    \n    # Calculate hitrate@3 for each ranker_id\n    ranker_hr3 = (\n        va_df.sort([\"ranker_id\", \"pred_score\"], descending=[False, True])\n        .group_by(\"ranker_id\", maintain_order=True)\n        .agg([\n            pl.col(\"selected\").head(3).max().alias(\"hit_top3\"),\n            pl.col(\"group_size\").first()\n        ])\n    )\n    \n    # Assign bins and calculate hitrate per bin\n    bin_centers = (bins[:-1] + bins[1:]) / 2  # Geometric mean would be more accurate for log scale\n    bin_indices = np.digitize(ranker_hr3['group_size'].to_numpy(), bins) - 1\n    \n    size_analysis = pl.DataFrame({\n        'bin_idx': bin_indices,\n        'bin_center': bin_centers[np.clip(bin_indices, 0, len(bin_centers)-1)],\n        'hit_top3': ranker_hr3['hit_top3']\n    }).group_by(['bin_idx', 'bin_center']).agg([\n        pl.col('hit_top3').mean().alias('hitrate3'),\n        pl.len().alias('n_groups')\n    ]).filter(pl.col('n_groups') >= 3).sort('bin_center')  # At least 3 groups per bin\n    \n    # Create combined figure\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), dpi=400)\n    \n    # Left plot: HitRate@k curves\n    # Create color gradient from blue to red for size groups\n    colors = ['black']  # All groups is black\n    for i in range(3):  # 3 size groups\n        t = i / 2  # 0, 0.5, 1\n        color = tuple(blue[j] * (1 - t) + red[j] * t for j in range(3))\n        colors.append(color)\n    \n    for (label, hitrates), color in zip(curves.items(), colors):\n        ax1.plot(k_values, hitrates, marker='o', label=label, color=color, markersize=3)\n    ax1.set_xlabel('k (top-k predictions)')\n    ax1.set_ylabel('HitRate@k')\n    ax1.set_title('HitRate@k by Group Size')\n    ax1.legend(fontsize=8)\n    ax1.grid(True, alpha=0.3)\n    ax1.set_xlim(0, 21)\n    ax1.set_ylim(-0.025, 1.025)\n    \n    # Right plot: HitRate@3 vs Group Size (log scale)\n    ax2.scatter(size_analysis['bin_center'], size_analysis['hitrate3'], s=30, alpha=0.6, color=blue)\n    ax2.set_xlabel('Group Size')\n    ax2.set_ylabel('HitRate@3')\n    ax2.set_title('HitRate@3 vs Group Size')\n    ax2.set_xscale('log')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n    # Summary\n    print(f\"HitRate@1: {curves['All groups (>10)'][0]:.3f}\")\n    print(f\"HitRate@3: {curves['All groups (>10)'][2]:.3f}\")\n    print(f\"HitRate@5: {curves['All groups (>10)'][4]:.3f}\")\n    print(f\"HitRate@10: {curves['All groups (>10)'][9]:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:55:10.869341Z","iopub.status.idle":"2025-08-16T08:55:10.869971Z","shell.execute_reply.started":"2025-08-16T08:55:10.869499Z","shell.execute_reply":"2025-08-16T08:55:10.869512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if model_type == \"xgb\":\n    submission_xgb = (\n        test.select(['Id', 'ranker_id'])\n        .with_columns(pl.Series('pred_score', xgb_model.predict(dtest_xgb)))\n        .with_columns(\n            pl.col('pred_score')\n            .rank(method='ordinal', descending=True)\n            .over('ranker_id')\n            .cast(pl.Int32)\n            .alias('selected')\n        )\n        .select(['Id', 'ranker_id', 'selected'])\n    )\n    submission_xgb.write_csv('submission.csv')\nelif model_type == \"lgb\":\n    submission_lgb = (\n    test.select(['Id', 'ranker_id'])\n    .with_columns(pl.Series('pred_score', lgb_model.predict(data_te)))\n    .with_columns(\n        pl.col('pred_score')\n        .rank(method='ordinal', descending=True)\n        .over('ranker_id')\n        .cast(pl.Int32)\n        .alias('selected')\n        )\n    .select(['Id', 'ranker_id', 'selected'])\n    )\n    submission_lgb.write_csv('submission.csv')\nelif model_type == \"ensemble\":\n    alpha = 0.6\n    submission_ens = (\n    test.select(['Id', 'ranker_id'])\n    .with_columns(pl.Series('pred_score', (alpha * lgb_model.predict(data_te)) + ((1 - alpha) * xgb_model.predict(dtest_xgb))))\n    .with_columns(\n        pl.col('pred_score')\n        .rank(method='ordinal', descending=True)\n        .over('ranker_id')\n        .cast(pl.Int32)\n        .alias('selected')\n        )\n    .select(['Id', 'ranker_id', 'selected'])\n    )\n    submission_ens.write_csv('submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T08:55:10.870763Z","iopub.status.idle":"2025-08-16T08:55:10.871293Z","shell.execute_reply.started":"2025-08-16T08:55:10.870905Z","shell.execute_reply":"2025-08-16T08:55:10.870918Z"}},"outputs":[],"execution_count":null}]}